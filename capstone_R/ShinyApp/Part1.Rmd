---
title: "part 1"
author: "jameel"
date: "2024-05-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(stringr)
library(dplyr)
library(ggplot2)
```

## Introduction

This report presents the initial exploratory analysis of the text data and outlines the plans for developing a prediction algorithm and a Shiny app. The goal is to demonstrate successful data loading, provide summary statistics, highlight interesting findings, and outline the steps for creating the prediction algorithm.

# Data Loading and Sampling

```{r data-loading}
# Function to sample lines from a large file
sample_file <- function(file_path, sample_fraction = 0.1) {
  con <- file(file_path, "r")
  lines <- readLines(con, warn = FALSE)
  close(con)
  
  sample_size <- ceiling(length(lines) * sample_fraction)
  sampled_lines <- sample(lines, sample_size)
  return(sampled_lines)
}

file_path <- "en_US.blogs.txt"  
sampled_lines <- sample_file(file_path, 0.1)
text_sample <- paste(sampled_lines, collapse = " ")

```

# Tokenization

```{r tokenization}
```{r tokenization}
# Tokenization function using stringr
tokenize <- function(text) {
  tokens <- unlist(str_extract_all(text, boundary("word")))
  return(tokens)
}

tokens <- tokenize(tolower(text_sample))

```

# Profanity Filtering

```{r profanity-filtering}
badwords_url <- "http://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
download.file(badwords_url, destfile = "bad-words.txt")
badwords <- readLines("bad-words.txt", encoding="UTF-8")

# Function to remove profane words from tokens
remove_profanity <- function(tokens, profane_words) {
  clean_tokens <- tokens[!tokens %in% profane_words]
  return(clean_tokens)
}

clean_tokens <- remove_profanity(tokens, badwords)

```

# Summary Statistics

```{r summary}
# Create a table of token frequencies
token_freq <- data.frame(table(clean_tokens))
colnames(token_freq) <- c("Token", "Frequency")
token_freq <- token_freq %>% arrange(desc(Frequency))
head(token_freq, 10)

```

# Visualization

```{r visualization}
# Plot the top 20 most frequent tokens
top_20_tokens <- token_freq[1:20, ]

ggplot(data = top_20_tokens, aes(x = reorder(Token, -Frequency), y = Frequency)) +
  geom_bar(stat = "identity") +
  xlab("Token") +
  ylab("Frequency") +
  ggtitle("Top 20 Most Frequent Tokens") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



## Interesting Findings 
The most frequent tokens are predominantly common words.
There are some variations in the frequency of different words, which could be insightful for building the prediction model.

# Plans for Prediction Algorithm

The prediction algorithm will leverage n-gram models to predict the next word based on the input text. The Shiny app will provide an interactive interface for users to input text and receive predictions in real-time. Future steps include:

	1.	Creating a comprehensive n-gram model.
	2.	Implementing the prediction algorithm.
	3.	Developing the Shiny app interface.
	4.	Testing and refining the app for performance and accuracy.
	
# Conclusion
This initial analysis demonstrates successful data loading, tokenization, and basic filtering. The next steps will focus on building and implementing the prediction algorithm and Shiny app.







